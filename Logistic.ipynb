{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5234d892-b520-4fba-86af-0720b191b57e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels counts in X_train 1879\n",
      "Total rows in test set 498121\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('./data/train.csv', sep='|')\n",
    "df_test = pd.read_csv('./data/test.csv', sep='|')\n",
    "\n",
    "new_df = df.dropna()\n",
    "new_df_test = df_test.dropna()\n",
    "\n",
    "new_df.insert(1, 'totalItemsScanned', new_df['scannedLineItemsPerSecond'] * new_df['totalScanTimeInSeconds'])\n",
    "new_df_test.insert(1, 'totalItemsScanned', new_df_test['scannedLineItemsPerSecond'] * new_df_test['totalScanTimeInSeconds'])\n",
    "\n",
    "y_train = new_df.pop('fraud')\n",
    "\n",
    "X_train = new_df.drop(columns=[\n",
    "    'scannedLineItemsPerSecond', 'lineItemVoidsPerPosition', 'valuePerSecond',\n",
    "    'quantityModifications', 'grandTotal'\n",
    "])\n",
    "X_test = new_df_test.drop(columns=[\n",
    "    'scannedLineItemsPerSecond', 'lineItemVoidsPerPosition', 'valuePerSecond',\n",
    "    'quantityModifications', 'grandTotal'\n",
    "])\n",
    "\n",
    "print('Labels counts in X_train',len(X_train))\n",
    "print('Total rows in test set',len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83673cf7-8762-4dc4-9769-55a1ec5d458e",
   "metadata": {},
   "source": [
    "**Chia tập dữ liệu**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "03c8a553-1512-454e-999a-0828daf92945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels counts in X_train: 1315\n",
      "Labels counts in X_test: 498121\n",
      "Labels counts in y_train: 1315\n",
      "Labels counts in y_test: 564\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n",
    "\n",
    "print('Labels counts in X_train:', len(X_train))\n",
    "print('Labels counts in X_test:', len(X_test))\n",
    "\n",
    "print('Labels counts in y_train:', len(y_train))\n",
    "print('Labels counts in y_test:', len(y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86450c1-ee4a-4819-94b5-098c5dad4e49",
   "metadata": {},
   "source": [
    "**Fit Mô Hình** \n",
    "non-penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8d8a5393-4cfd-49be-857f-27e0cefb15fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Tạo mô hình Logistic Regression không sử dụng penalty\n",
    "lr_model = LogisticRegression(penalty=None, max_iter=100000, random_state=1)\n",
    "\n",
    "# Fit mô hình với train data\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Dự đoán trên test data dùng mô hình đã train\n",
    "y_pred_val = lr_model.predict(X_test)\n",
    "y_pred_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad1b823c-5db9-4778-b991-b69ac7e8dbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows with fraud predict:  (24071, 1)\n",
      "Rows with non-fraud predict:  (474050, 1)\n"
     ]
    }
   ],
   "source": [
    "y_pred_val = lr_model.predict(X_test)\n",
    "\n",
    "# Create result DataFrame\n",
    "result_df = pd.DataFrame({\"fraud\": y_pred_val})\n",
    "\n",
    "print('Rows with fraud predict: ',result_df[result_df['fraud'] == 1].shape)\n",
    "print('Rows with non-fraud predict: ',result_df[result_df['fraud'] == 0].shape)\n",
    "\n",
    "# Lưu kết quả ra file CSV\n",
    "result_df.to_csv(\"./result/resultLogistic.csv\", index=False)\n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8cf74c-5a24-4d28-a19e-6abf2c0432df",
   "metadata": {},
   "source": [
    "### Đánh giá mô hình\n",
    "\n",
    "Ta sẽ đánh giá mô hình dùng confusion matrix (CM) và các độ đo accuracy, f1_score, f1_micro, f1_macro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a5d856b-5104-4738-a909-b72de710d4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĐÁNH GIÁ MÔ HÌNH LOGISTIC REGRESSION\n",
      "The validation confusion matrix is:\n",
      " [[526   3]\n",
      " [  2  33]]\n",
      "The validation accuracy is: 0.9911347517730497\n",
      "The validation f1_score is: 0.9295774647887323\n",
      "The validation f1_micro is: 0.9911347517730497\n",
      "The validation f1_macro is: 0.9624235479099763\n"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "y_pred_val = lr_model.predict(X_val)\n",
    "\n",
    "print('ĐÁNH GIÁ MÔ HÌNH LOGISTIC REGRESSION')\n",
    "# Tính confusion matrix trên tập validation\n",
    "cm_val = metrics.confusion_matrix(y_true=y_val, y_pred=y_pred_val)\n",
    "print(f'The validation confusion matrix is:\\n {cm_val}')\n",
    "\n",
    "# Tính accuracy trên tập validation\n",
    "accuracy = metrics.accuracy_score(y_true=y_val, y_pred=y_pred_val)\n",
    "print(f'The validation accuracy is: {accuracy}')\n",
    "\n",
    "# Tính f1_score trên tập validation\n",
    "f1_score = metrics.f1_score(y_true=y_val, y_pred=y_pred_val, average='binary')\n",
    "print(f'The validation f1_score is: {f1_score}')\n",
    "\n",
    "# Tính f1_micro trên tập validation\n",
    "f1_micro = metrics.f1_score(y_true=y_val, y_pred=y_pred_val, average='micro')\n",
    "print(f'The validation f1_micro is: {f1_micro}')\n",
    "\n",
    "# Tính f1_macro trên tập validation\n",
    "f1_macro = metrics.f1_score(y_true=y_val, y_pred=y_pred_val, average='macro')\n",
    "print(f'The validation f1_macro is: {f1_macro}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3801426c-7b61-498a-9734-aa7db8681354",
   "metadata": {},
   "source": [
    "**L2 Penalty**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f21c279b-f3d5-4ddc-8d08-598e12b5d492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels counts in X_train 1879\n",
      "Total rows in test set 498121\n",
      "Labels counts in X_train: 1315\n",
      "Labels counts in X_test: 498121\n",
      "Labels counts in y_train: 1315\n",
      "Labels counts in y_test: 564\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.read_csv('./data/train.csv', sep='|')\n",
    "df_test = pd.read_csv('./data/test.csv', sep='|')\n",
    "\n",
    "new_df = df.dropna()\n",
    "new_df_test = df_test.dropna()\n",
    "\n",
    "new_df.insert(1, 'totalItemsScanned', new_df['scannedLineItemsPerSecond'] * new_df['totalScanTimeInSeconds'])\n",
    "new_df_test.insert(1, 'totalItemsScanned', new_df_test['scannedLineItemsPerSecond'] * new_df_test['totalScanTimeInSeconds'])\n",
    "\n",
    "y_train = new_df.pop('fraud')\n",
    "\n",
    "X_train = new_df.drop(columns=[\n",
    "    'scannedLineItemsPerSecond', 'lineItemVoidsPerPosition', 'valuePerSecond',\n",
    "    'quantityModifications', 'grandTotal'\n",
    "])\n",
    "X_test = new_df_test.drop(columns=[\n",
    "    'scannedLineItemsPerSecond', 'lineItemVoidsPerPosition', 'valuePerSecond',\n",
    "    'quantityModifications', 'grandTotal'\n",
    "])\n",
    "\n",
    "print('Labels counts in X_train',len(X_train))\n",
    "print('Total rows in test set',len(X_test))\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=1)\n",
    "\n",
    "print('Labels counts in X_train:', len(X_train))\n",
    "print('Labels counts in X_test:', len(X_test))\n",
    "\n",
    "print('Labels counts in y_train:', len(y_train))\n",
    "print('Labels counts in y_test:', len(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8308aba6-7c0f-45fc-b939-8ca2f6b49ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Huấn luyện mô hình Logistic Regression với L2 penalty (ridge) và C = 0.01\n",
    "logit_ridge = LogisticRegression(penalty='l2', C=0.01, max_iter=100000).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d16a6c9-d765-4782-a1c1-71125a9171c7",
   "metadata": {},
   "source": [
    "+++ Dự đoán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be431d73-5b10-49c3-b696-27b6db6ac2e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ĐÁNH GIÁ MÔ HÌNH LOGISTIC REGRESSION\n",
      "The validation confusion matrix is:\n",
      " [[529   0]\n",
      " [ 29   6]]\n",
      "The validation accuracy is: 0.9485815602836879\n",
      "The validation f1_score is: 0.2926829268292683\n",
      "The validation f1_micro is: 0.9485815602836879\n",
      "The validation f1_macro is: 0.6330019969932911\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logit_ridge = LogisticRegression(penalty='l2', C=0.01, max_iter=100000).fit(X_train, y_train)\n",
    "# Dự đoán xác suất dựa trên mô hình logistic không có penalty và mô hình logistic có L2 penalty\n",
    "y_pred_logit_ridge = logit_ridge.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Thiết lập ngưỡng (threshold)\n",
    "threshold = 0.5\n",
    "y_pred_logit_ridge = logit_ridge.predict(X_val)\n",
    "\n",
    "print('ĐÁNH GIÁ MÔ HÌNH LOGISTIC REGRESSION')\n",
    "# Tính confusion matrix trên tập validation\n",
    "cm_val = metrics.confusion_matrix(y_true=y_val, y_pred=y_pred_logit_ridge)\n",
    "print(f'The validation confusion matrix is:\\n {cm_val}')\n",
    "\n",
    "# Tính accuracy trên tập validation\n",
    "accuracy = metrics.accuracy_score(y_true=y_val, y_pred=y_pred_logit_ridge)\n",
    "print(f'The validation accuracy is: {accuracy}')\n",
    "\n",
    "# Tính f1_score trên tập validation\n",
    "f1_score = metrics.f1_score(y_true=y_val, y_pred=y_pred_logit_ridge, average='binary')\n",
    "print(f'The validation f1_score is: {f1_score}')\n",
    "\n",
    "# Tính f1_micro trên tập validation\n",
    "f1_micro = metrics.f1_score(y_true=y_val, y_pred=y_pred_logit_ridge, average='micro')\n",
    "print(f'The validation f1_micro is: {f1_micro}')\n",
    "\n",
    "# Tính f1_macro trên tập validation\n",
    "f1_macro = metrics.f1_score(y_true=y_val, y_pred=y_pred_logit_ridge, average='macro')\n",
    "print(f'The validation f1_macro is: {f1_macro}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35214187-01ad-435d-92be-c9ed74dbf202",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
